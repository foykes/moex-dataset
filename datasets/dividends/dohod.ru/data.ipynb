{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pandas as pd, sys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "df_overview_full = pd.DataFrame()\n",
    "df_years_full = pd.DataFrame()\n",
    "df_each_full = pd.DataFrame()\n",
    "\n",
    "current_path = sys.path[0]\n",
    "\n",
    "url = 'https://www.dohod.ru/ik/analytics/dividend'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Составление общего списка\n",
    "def get_main (url):\n",
    "    \n",
    "    html = requests.get(url, headers=headers).content\n",
    "    df_list = pd.read_html(html)\n",
    "    df_mainpage = df_list[-1]\n",
    "\n",
    "    # очистка от ненужных колонок\n",
    "    bad_columns = []\n",
    "    for i in list(df_mainpage.columns):\n",
    "        if \"Unnamed:\" in i or \"Капитализация, sorting\" in i:\n",
    "            bad_columns.append(i)\n",
    "\n",
    "    # print(bad_columns)\n",
    "    df_mainpage.drop(columns=bad_columns, inplace=True)\n",
    "\n",
    "    # len(df_mainpage)\n",
    "    if len(df_mainpage) > 0:\n",
    "        df_mainpage.to_excel('./obzor_vsex_divs_tickerov.xlsx')\n",
    "    else:\n",
    "        print('Не удалось скачать актуальные данные о дивах с доход.ру')\n",
    "    return df_mainpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_info (url):\n",
    "\n",
    "    df_overview_full = pd.DataFrame()\n",
    "    df_years_full = pd.DataFrame()\n",
    "    df_each_full = pd.DataFrame()\n",
    "\n",
    "    ### Сбор всех ссылок на страницы\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        data.append(str(link.get('href')))\n",
    "\n",
    "\n",
    "    data = list(set(data))\n",
    "\n",
    "    url_list = []\n",
    "\n",
    "    for i in data:\n",
    "        if \"/ik/analytics/dividend/\" in i and \".pdf\" not in i:\n",
    "            url = \"https://www.dohod.ru\" + i\n",
    "            url_list.append(url)\n",
    "    \n",
    "\n",
    "    print(\"Количество страниц для обхода: {}\".format(len(url_list)))\n",
    "\n",
    "\n",
    "    ### Обход каждой страницы\n",
    "\n",
    "    b = []\n",
    "    for url in url_list:\n",
    "        html = requests.get(url, headers=headers).content\n",
    "        try:\n",
    "            df_list = pd.read_html(html)\n",
    "        except:\n",
    "            df_list = pd.DataFrame()\n",
    "            print('Не смог найти таблицы на этой странице: {}',format(url))\n",
    "\n",
    "        a = len(df_list)\n",
    "        b.append(a)\n",
    "\n",
    "        if len(df_list) == 2:\n",
    "            df = df_list[0]\n",
    "            df.columns = df.iloc[1]\n",
    "            df.reset_index(inplace=True)\n",
    "            df = df.drop (index= 1)\n",
    "            df = df[['текущая доходность','доля от прибыли','индекс DSI']]\n",
    "            df['page_url'] = url\n",
    "            df['ticker'] = url.split(r'/')[-1].upper()\n",
    "            df_overview_full = pd.concat([df_overview_full,df])\n",
    "\n",
    "            df = df_list[1]\n",
    "            df['page_url'] = url\n",
    "            df['ticker'] = url.split(r'/')[-1].upper()\n",
    "            df_years_full = pd.concat([df_years_full,df])\n",
    "\n",
    "        elif len(df_list) == 3:\n",
    "            df = df_list[0]\n",
    "            df.columns = df.iloc[1]\n",
    "            df.reset_index(inplace=True)\n",
    "            df = df.drop (index= 1)\n",
    "            df = df[['текущая доходность','доля от прибыли','индекс DSI']]\n",
    "            df['page_url'] = url\n",
    "            df['ticker'] = url.split(r'/')[-1].upper()\n",
    "            df_overview_full = pd.concat([df_overview_full,df])\n",
    "\n",
    "            df = df_list[1]\n",
    "            df['page_url'] = url\n",
    "            df['ticker'] = url.split(r'/')[-1].upper()\n",
    "            df_years_full = pd.concat([df_years_full,df])\n",
    "\n",
    "            df = df_list[2]\n",
    "            df['page_url'] = url\n",
    "            df['ticker'] = url.split(r'/')[-1].upper()\n",
    "            df_each_full = pd.concat([df_each_full,df])\n",
    "\n",
    "\n",
    "    ## Статистика для отладки. Сколько разных табличек спарсено\n",
    "    o = list(set(b))\n",
    "\n",
    "    for i in o:\n",
    "        print(\"Тип {}. Количество: {}\".format(i, b.count(i)))\n",
    "        print(\"___\")\n",
    "    \n",
    "    return df_overview_full, df_years_full, df_each_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mainpage = get_main(url)\n",
    "df_mainpage.to_excel('./main_page.xlsx')\n",
    "df_mainpage.to_csv('./main_page.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество страниц для обхода: 122\n",
      "Тип 2. Количество: 7\n",
      "7\n",
      "___\n",
      "Тип 3. Количество: 115\n",
      "115\n",
      "___\n"
     ]
    }
   ],
   "source": [
    "df_overview_full, df_years_full, df_each_full = get_page_info (url)\n",
    "df_overview_full.to_excel('./rate_for_each_ticker.xlsx')\n",
    "df_overview_full.to_csv('./rate_for_each_ticker.csv')\n",
    "\n",
    "df_years_full.to_excel('./sum_each_year_divs.xlsx')\n",
    "df_years_full.to_csv('./sum_each_year_divs.csv')\n",
    "\n",
    "df_each_full.to_excel('./all_payments.xlsx')\n",
    "df_each_full.to_csv('./all_payments.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
